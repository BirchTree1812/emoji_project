{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0b8c47",
   "metadata": {
    "papermill": {
     "duration": 0.004556,
     "end_time": "2025-12-01T11:50:49.031024",
     "exception": false,
     "start_time": "2025-12-01T11:50:49.026468",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Emoji Classification Project - Computer Vision**\n",
    "\n",
    "### Team Members : `Daniil NOTKIN`, `Yuhan SU` & `Yassine ERRAJI`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7386d4",
   "metadata": {},
   "source": [
    "## *Importations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2cdae888",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-01T11:50:49.039302Z",
     "iopub.status.busy": "2025-12-01T11:50:49.038980Z",
     "iopub.status.idle": "2025-12-01T11:51:05.852568Z",
     "shell.execute_reply": "2025-12-01T11:51:05.851194Z"
    },
    "papermill": {
     "duration": 16.819946,
     "end_time": "2025-12-01T11:51:05.854417",
     "exception": false,
     "start_time": "2025-12-01T11:50:49.034471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from skimage import io, color\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6947512d",
   "metadata": {
    "papermill": {
     "duration": 0.003767,
     "end_time": "2025-12-01T11:51:35.134476",
     "exception": false,
     "start_time": "2025-12-01T11:51:35.130709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## *Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "10c3126c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:35.144206Z",
     "iopub.status.busy": "2025-12-01T11:51:35.143610Z",
     "iopub.status.idle": "2025-12-01T11:51:35.153447Z",
     "shell.execute_reply": "2025-12-01T11:51:35.152366Z"
    },
    "papermill": {
     "duration": 0.01691,
     "end_time": "2025-12-01T11:51:35.155544",
     "exception": false,
     "start_time": "2025-12-01T11:51:35.138634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_single_image(path, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Load, preprocess, and resize an image to the target size.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to the image file\n",
    "        target_size: Tuple (height, width) to resize the image to. Default is (224, 224) for EfficientNetB0.\n",
    "    \n",
    "    Returns:\n",
    "        Normalized image array as float32 in range [0, 1]\n",
    "    \"\"\"\n",
    "    img = Image.open(path).convert(\"RGBA\")\n",
    "    img = np.array(img)\n",
    "\n",
    "    # RGBA -> RGB\n",
    "    if img.ndim == 3 and img.shape[2] == 4:\n",
    "        img = color.rgba2rgb(img) \n",
    "    # grayscale -> RGB\n",
    "    elif img.ndim == 2:\n",
    "        img = np.stack([img] * 3, axis=-1)\n",
    "\n",
    "    # uint8 conversion\n",
    "    if img.dtype != np.uint8:\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "\n",
    "    # Resize to target size for consistent batch dimensions\n",
    "    # Convert back to PIL, resize, then convert to numpy\n",
    "    img_pil = Image.fromarray((img * 255).astype(np.uint8) if img.dtype != np.uint8 else img)\n",
    "    img_pil = img_pil.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    img = np.array(img_pil)\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    return img.astype(\"float32\") / 255.0\n",
    "\n",
    "\n",
    "def imageLoader(files, labels, batch_size):\n",
    "    \"\"\"\n",
    "    This will create a generator for learning by batches.\n",
    "    \"\"\"\n",
    "    L = len(files)\n",
    "\n",
    "    while True: \n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "\n",
    "        while batch_start < L:\n",
    "            limit = min(batch_end, L)\n",
    "\n",
    "            # Batch files (slices)\n",
    "            batch_files = files[batch_start:limit]\n",
    "\n",
    "            X_batch = []\n",
    "            y_batch = []\n",
    "\n",
    "            for f in batch_files:\n",
    "                img_id = Path(f).stem       # nom sans extension\n",
    "                img = load_single_image(f)\n",
    "                X_batch.append(img)\n",
    "                y_batch.append(labels[img_id])\n",
    "\n",
    "            X = np.stack(X_batch)\n",
    "            Y = np.array(y_batch)\n",
    "\n",
    "            yield X, Y\n",
    "\n",
    "            batch_start += batch_size\n",
    "            batch_end += batch_size\n",
    "\n",
    "# source code https://stackoverflow.com/questions/47200146/keras-load-images-batch-wise-for-large-dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d078b9",
   "metadata": {
    "papermill": {
     "duration": 0.004142,
     "end_time": "2025-12-01T11:51:35.164144",
     "exception": false,
     "start_time": "2025-12-01T11:51:35.160002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "50a6fe9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:35.173666Z",
     "iopub.status.busy": "2025-12-01T11:51:35.172746Z",
     "iopub.status.idle": "2025-12-01T11:51:35.177789Z",
     "shell.execute_reply": "2025-12-01T11:51:35.176634Z"
    },
    "papermill": {
     "duration": 0.011591,
     "end_time": "2025-12-01T11:51:35.179536",
     "exception": false,
     "start_time": "2025-12-01T11:51:35.167945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = Path(\"data\")\n",
    "if not PATH.exists():\n",
    "    raise FileNotFoundError(f\"Data folder not found at {PATH.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb7a5821",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:35.190065Z",
     "iopub.status.busy": "2025-12-01T11:51:35.188956Z",
     "iopub.status.idle": "2025-12-01T11:51:41.609047Z",
     "shell.execute_reply": "2025-12-01T11:51:41.607769Z"
    },
    "papermill": {
     "duration": 6.427634,
     "end_time": "2025-12-01T11:51:41.611072",
     "exception": false,
     "start_time": "2025-12-01T11:51:35.183438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = PATH / \"train\"\n",
    "if not train_dir.exists():\n",
    "    raise FileNotFoundError(f\"Train directory not found at {train_dir.resolve()}\")\n",
    "train_files = sorted([str(p) for p in train_dir.iterdir() if p.is_file()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a0172a2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:41.620701Z",
     "iopub.status.busy": "2025-12-01T11:51:41.620397Z",
     "iopub.status.idle": "2025-12-01T11:51:47.752941Z",
     "shell.execute_reply": "2025-12-01T11:51:47.751917Z"
    },
    "papermill": {
     "duration": 6.139414,
     "end_time": "2025-12-01T11:51:47.754931",
     "exception": false,
     "start_time": "2025-12-01T11:51:41.615517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dir = PATH / \"test\"\n",
    "if not test_dir.exists():\n",
    "    raise FileNotFoundError(f\"Test directory not found at {test_dir.resolve()}\")\n",
    "test_files = sorted([str(p) for p in test_dir.iterdir() if p.is_file()])\n",
    "test_ids = [Path(f).stem for f in test_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "675589f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:47.766730Z",
     "iopub.status.busy": "2025-12-01T11:51:47.766404Z",
     "iopub.status.idle": "2025-12-01T11:51:47.793766Z",
     "shell.execute_reply": "2025-12-01T11:51:47.792429Z"
    },
    "papermill": {
     "duration": 0.035073,
     "end_time": "2025-12-01T11:51:47.795604",
     "exception": false,
     "start_time": "2025-12-01T11:51:47.760531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample label dict keys: ['00001', '00002', '00003', '00004', '00005']\n",
      "Sample image stems: ['00001', '00002', '00003', '00004', '00005']\n"
     ]
    }
   ],
   "source": [
    "labels_csv = PATH / \"train_labels.csv\"\n",
    "if not labels_csv.exists():\n",
    "    raise FileNotFoundError(f\"Labels file not found at {labels_csv.resolve()}\")\n",
    "\n",
    "y_train_df = pd.read_csv(labels_csv)\n",
    "\n",
    "# Convert Id column to string format to match image filenames\n",
    "# This handles cases where IDs might be integers (1) or strings (\"00001\")\n",
    "y_train_df[\"Id\"] = y_train_df[\"Id\"].astype(str)\n",
    "\n",
    "# If IDs are purely numeric strings (e.g., \"1\", \"2\"), pad them to match filenames\n",
    "if all(x.isdigit() for x in y_train_df[\"Id\"]):\n",
    "    # Determine padding length from the image filenames\n",
    "    sample_stems = [Path(f).stem for f in train_files]\n",
    "    max_padding = max(len(s) for s in sample_stems)\n",
    "    y_train_df[\"Id\"] = y_train_df[\"Id\"].str.zfill(max_padding)\n",
    "\n",
    "y_train_dct = dict(zip(y_train_df[\"Id\"], y_train_df[\"Label\"]))\n",
    "\n",
    "# Debug: show sample IDs\n",
    "print(f\"Sample label dict keys: {list(y_train_dct.keys())[:5]}\")\n",
    "print(f\"Sample image stems: {[Path(f).stem for f in train_files[:5]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7fda3bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:47.805270Z",
     "iopub.status.busy": "2025-12-01T11:51:47.804665Z",
     "iopub.status.idle": "2025-12-01T11:51:47.811792Z",
     "shell.execute_reply": "2025-12-01T11:51:47.810132Z"
    },
    "papermill": {
     "duration": 0.014286,
     "end_time": "2025-12-01T11:51:47.813901",
     "exception": false,
     "start_time": "2025-12-01T11:51:47.799615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen = imageLoader(\n",
    "    files=train_files,\n",
    "    labels=y_train_dct,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e7aa7",
   "metadata": {
    "papermill": {
     "duration": 0.003681,
     "end_time": "2025-12-01T11:51:47.821889",
     "exception": false,
     "start_time": "2025-12-01T11:51:47.818208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c023555",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:47.834550Z",
     "iopub.status.busy": "2025-12-01T11:51:47.834131Z",
     "iopub.status.idle": "2025-12-01T11:51:47.842502Z",
     "shell.execute_reply": "2025-12-01T11:51:47.841381Z"
    },
    "papermill": {
     "duration": 0.01805,
     "end_time": "2025-12-01T11:51:47.844647",
     "exception": false,
     "start_time": "2025-12-01T11:51:47.826597",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model Architecture & Strategy\n",
    "\n",
    "We use **Transfer Learning with EfficientNetB0** for several reasons:\n",
    "\n",
    "1. **Pre-trained Weights**: EfficientNetB0 is trained on ImageNet with 1.3M images across 1000 classes. These learned features (edge detection, texture recognition, etc.) transfer well to emoji classification.\n",
    "\n",
    "2. **Efficiency-Accuracy Trade-off**: EfficientNetB0 offers excellent accuracy (~77% top-1 on ImageNet) while remaining lightweight (~5.3M parameters), enabling faster training and inference.\n",
    "\n",
    "3. **Emoji Domain**: Emojis are stylized, small images where extracting visual features (shapes, colors, patterns) is crucial - areas where CNNs excel. Transfer learning accelerates learning by leveraging pre-trained feature extractors.\n",
    "\n",
    "4. **Data Augmentation**: We apply random rotations, shifts, and zoom to increase training diversity and improve generalization on unseen emoji variations.\n",
    "\n",
    "5. **Regularization**: Dropout prevents overfitting; we freeze early layers and only fine-tune later layers to preserve learned features while adapting to emoji-specific patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec2a6d84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:47.854144Z",
     "iopub.status.busy": "2025-12-01T11:51:47.853758Z",
     "iopub.status.idle": "2025-12-01T11:51:47.864613Z",
     "shell.execute_reply": "2025-12-01T11:51:47.863411Z"
    },
    "papermill": {
     "duration": 0.018326,
     "end_time": "2025-12-01T11:51:47.867010",
     "exception": false,
     "start_time": "2025-12-01T11:51:47.848684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping created: 7 classes\n",
      "Sample mapping: [('samsung', 0), ('apple', 1), ('facebook', 2), ('google', 3), ('messenger', 4)]\n",
      "Model created with 4,411,306 total parameters\n",
      "Number of emoji classes: 7\n",
      "Training samples: 9879\n",
      "Epoch 1/30\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 350ms/step - accuracy: 0.1877 - loss: 2.7074\n",
      "Epoch 2/30\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 359ms/step - accuracy: 0.1841 - loss: 1.8906\n",
      "Epoch 3/30\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 362ms/step - accuracy: 0.1866 - loss: 1.8553\n",
      "Epoch 4/30\n",
      "\u001b[1m281/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 373ms/step - accuracy: 0.1997 - loss: 1.8502"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 150\u001b[39m\n\u001b[32m    143\u001b[39m early_stopping = EarlyStopping(\n\u001b[32m    144\u001b[39m     monitor=\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m,  \u001b[38;5;66;03m# Monitor training loss\u001b[39;00m\n\u001b[32m    145\u001b[39m     patience=\u001b[32m3\u001b[39m,      \u001b[38;5;66;03m# Stop if no improvement for 3 epochs\u001b[39;00m\n\u001b[32m    146\u001b[39m     restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    147\u001b[39m )\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# Train for maximum 30 epochs (likely to stop earlier with early stopping)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_files\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    156\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining completed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# STEP 5: Generate Predictions on Test Set\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CV_Emojis/emoji_project/venv/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CV_Emojis/emoji_project/venv/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CV_Emojis/emoji_project/venv/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CV_Emojis/emoji_project/venv/lib/python3.13/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CV_Emojis/emoji_project/venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CV_Emojis/emoji_project/venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CV_Emojis/emoji_project/venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CV_Emojis/emoji_project/venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CV_Emojis/emoji_project/venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CV_Emojis/emoji_project/venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CV_Emojis/emoji_project/venv/lib/python3.13/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CV_Emojis/emoji_project/venv/lib/python3.13/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# We start by listing the existing labels\n",
    "unique_labels = y_train_df[\"Label\"].unique().tolist()\n",
    "\n",
    "# Create a mapping from emoji label (string) to class index (integer)\n",
    "# This is required because the model uses sparse_categorical_crossentropy which expects integer labels\n",
    "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "index_to_label = {idx: label for label, idx in label_to_index.items()}\n",
    "\n",
    "# Convert the label dictionary from string labels to integer indices\n",
    "y_train_dct_indexed = {img_id: label_to_index[label] for img_id, label in y_train_dct.items()}\n",
    "\n",
    "print(f\"Label mapping created: {len(label_to_index)} classes\")\n",
    "print(f\"Sample mapping: {list(label_to_index.items())[:5]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Define Advanced Data Augmentation\n",
    "# ============================================================================\n",
    "# Data augmentation artificially increases training data diversity by applying\n",
    "# random transformations. This helps the model generalize better to variations\n",
    "# in emoji styles and orientations.\n",
    "augmentation = ImageDataGenerator(\n",
    "    rotation_range=20,           # Random rotations up to 20 degrees\n",
    "    width_shift_range=0.1,       # Horizontal shift by 10%\n",
    "    height_shift_range=0.1,      # Vertical shift by 10%\n",
    "    zoom_range=0.15,             # Random zoom between 0.85x and 1.15x\n",
    "    horizontal_flip=True,        # Mirror images (valid for many emojis)\n",
    "    fill_mode='nearest'          # Fill pixels using nearest neighbor\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Create Modified Image Loader with Data Augmentation\n",
    "# ============================================================================\n",
    "def imageLoader_augmented(files, labels, batch_size, augment=True):\n",
    "    \"\"\"\n",
    "    Enhanced image loader with optional data augmentation.\n",
    "    Augmentation is applied during training but disabled for test/validation.\n",
    "    \n",
    "    Args:\n",
    "        files: List of image file paths\n",
    "        labels: Dictionary mapping image IDs to integer class indices\n",
    "        batch_size: Batch size for training\n",
    "        augment: Whether to apply data augmentation\n",
    "    \"\"\"\n",
    "    L = len(files)\n",
    "    while True: \n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        while batch_start < L:\n",
    "            limit = min(batch_end, L)\n",
    "            batch_files = files[batch_start:limit]\n",
    "            \n",
    "            X_batch = []\n",
    "            y_batch = []\n",
    "            for f in batch_files:\n",
    "                img_id = Path(f).stem\n",
    "                \n",
    "                # Robustly handle ID lookups: try original, then convert to int if numeric\n",
    "                if img_id not in labels:\n",
    "                    try:\n",
    "                        # Try converting to integer then back with padding\n",
    "                        img_id_int = int(img_id)\n",
    "                        # Find the correct padded version\n",
    "                        padded_versions = [k for k in labels.keys() if int(k) == img_id_int]\n",
    "                        if padded_versions:\n",
    "                            img_id = padded_versions[0]\n",
    "                        else:\n",
    "                            print(f\"Warning: Could not find label for {img_id}\")\n",
    "                            continue\n",
    "                    except ValueError:\n",
    "                        print(f\"Warning: Could not find label for {img_id}\")\n",
    "                        continue\n",
    "                \n",
    "                img = load_single_image(f)\n",
    "                X_batch.append(img)\n",
    "                y_batch.append(labels[img_id])  # Now this is an integer index\n",
    "            \n",
    "            if len(X_batch) > 0:  # Only yield non-empty batches\n",
    "                X = np.stack(X_batch)\n",
    "                Y = np.array(y_batch, dtype=np.int32)  # Ensure integer dtype\n",
    "                \n",
    "                # Apply augmentation only if requested (training phase)\n",
    "                if augment:\n",
    "                    X = next(augmentation.flow(X, Y, batch_size=len(X), shuffle=False))[0]\n",
    "                \n",
    "                yield X, Y\n",
    "            \n",
    "            batch_start += batch_size\n",
    "            batch_end += batch_size\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Build Transfer Learning Model\n",
    "# ============================================================================\n",
    "# Load EfficientNetB0 with ImageNet weights\n",
    "base_model = EfficientNetB0(\n",
    "    weights='imagenet',\n",
    "    include_top=False,           # Remove classification head, we'll add custom layers\n",
    "    input_shape=(224, 224, 3)    # Standard size for pre-trained model\n",
    ")\n",
    "\n",
    "# Freeze early layers: these extract generic features (edges, textures)\n",
    "# that are useful across domains. We only fine-tune later layers.\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build custom classification head\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),      # Reduce spatial dimensions (7x7 -> 1D)\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer='l2'),  # Learn emoji-specific features\n",
    "    layers.Dropout(0.5),                  # Prevent overfitting (randomly drop 50% units)\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer='l2'),  # Second learned layer\n",
    "    layers.Dropout(0.3),                  # Lighter dropout for stability\n",
    "    layers.Dense(len(unique_labels), activation='softmax')  # Output: probability per class\n",
    "])\n",
    "\n",
    "# Compile with Adam optimizer (adaptive learning rates) and categorical crossentropy\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',  # For integer labels (0, 1, 2, ...)\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Model created with {model.count_params():,} total parameters\")\n",
    "print(f\"Number of emoji classes: {len(unique_labels)}\")\n",
    "print(f\"Training samples: {len(train_files)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Train the Model\n",
    "# ============================================================================\n",
    "# Using the image generator for training with INDEXED labels (integers)\n",
    "train_generator = imageLoader_augmented(\n",
    "    files=train_files,\n",
    "    labels=y_train_dct_indexed,  # Use indexed labels (integers, not strings)\n",
    "    batch_size=32,\n",
    "    augment=True  # Enable augmentation for training\n",
    ")\n",
    "\n",
    "# Early stopping: if validation accuracy plateaus, stop training to save time\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss',  # Monitor training loss\n",
    "    patience=3,      # Stop if no improvement for 3 epochs\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train for maximum 30 epochs (likely to stop earlier with early stopping)\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_files) // 32,\n",
    "    epochs=30,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Generate Predictions on Test Set\n",
    "# ============================================================================\n",
    "def predict_with_my_amazing_model(test_files):\n",
    "    \"\"\"\n",
    "    Generate emoji predictions using the trained EfficientNetB0 model.\n",
    "    \n",
    "    Process:\n",
    "    1. Load and preprocess each test image\n",
    "    2. Feed through the trained model\n",
    "    3. Extract the class with highest probability\n",
    "    4. Map class index back to emoji label string\n",
    "    \n",
    "    Returns:\n",
    "        predictions: numpy array of predicted emoji labels (strings)\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    # Process in batches for efficiency\n",
    "    for i in range(0, len(test_files), 32):\n",
    "        batch_files = test_files[i:min(i+32, len(test_files))]\n",
    "        X_batch = []\n",
    "        \n",
    "        for f in batch_files:\n",
    "            img = load_single_image(f)\n",
    "            X_batch.append(img)\n",
    "        \n",
    "        X = np.stack(X_batch)\n",
    "        \n",
    "        # Get probability distribution over classes\n",
    "        probs = model.predict(X, verbose=0)\n",
    "        \n",
    "        # Get predicted class index (highest probability)\n",
    "        predicted_indices = np.argmax(probs, axis=1)\n",
    "        \n",
    "        # Map class indices back to emoji label strings\n",
    "        batch_predictions = [index_to_label[idx] for idx in predicted_indices]\n",
    "        predictions.extend(batch_predictions)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "y_test_pred = predict_with_my_amazing_model(test_files)\n",
    "print(f\"Predictions generated for {len(y_test_pred)} test samples\")\n",
    "print(f\"Unique predictions: {np.unique(y_test_pred)}\")\n",
    "y_test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31300ec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:47.877760Z",
     "iopub.status.busy": "2025-12-01T11:51:47.876306Z",
     "iopub.status.idle": "2025-12-01T11:51:47.905692Z",
     "shell.execute_reply": "2025-12-01T11:51:47.904276Z"
    },
    "papermill": {
     "duration": 0.03674,
     "end_time": "2025-12-01T11:51:47.907893",
     "exception": false,
     "start_time": "2025-12-01T11:51:47.871153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>whatsapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>mozilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10004</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005</td>\n",
       "      <td>facebook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id     Label\n",
       "0  10001    google\n",
       "1  10002  whatsapp\n",
       "2  10003   mozilla\n",
       "3  10004     apple\n",
       "4  10005  facebook"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ids_sr = pd.Series(test_ids, name=\"Id\")\n",
    "y_test_pred_sr = pd.Series(y_test_pred, name=\"Label\")\n",
    "submission_df = pd.concat([test_ids_sr, y_test_pred_sr], axis=1)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf431098",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:47.918697Z",
     "iopub.status.busy": "2025-12-01T11:51:47.918362Z",
     "iopub.status.idle": "2025-12-01T11:51:47.939475Z",
     "shell.execute_reply": "2025-12-01T11:51:47.938082Z"
    },
    "papermill": {
     "duration": 0.02915,
     "end_time": "2025-12-01T11:51:47.941734",
     "exception": false,
     "start_time": "2025-12-01T11:51:47.912584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14708949,
     "sourceId": 124850,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 67.667405,
   "end_time": "2025-12-01T11:51:50.835106",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-01T11:50:43.167701",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
